# -*- coding: utf-8 -*-
"""Neural_Style_Transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/devandrepascoa/Colab_Notebooks/blob/master/Neural_Style_Transfer.ipynb
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
import PIL.Image
from PIL import Image,ImageOps
from moviepy.editor import *
import moviepy
import functools
import pygame
import IPython.display as display
import moviepy.video.io.ffmpeg_writer as ffmpeg_writer

class Image_Utils():

    @staticmethod
    def load_img(img_path):
        img = tf.io.read_file(img_path)
        img = tf.image.decode_image(img, channels=3)
        img = tf.image.convert_image_dtype(img, tf.float32)

        return img

    @staticmethod
    def download_img(img):
        file_name = 'stylized-image.png'
        img.save(file_name)

        try:
            from google.colab import files
        except ImportError:
            pass
        else:
            files.download(file_name)

    @staticmethod
    def constrain_img(img, max_dim=512):
        shape = tf.cast(tf.shape(img)[:-1], tf.float32)
        long_dim = max(shape)
        scale = max_dim / long_dim

        new_shape = tf.cast(shape * scale, tf.int32)

        img = tf.image.resize(img, new_shape)
        img = img[tf.newaxis, :]
        return img

    @staticmethod
    def show_img(img, title=None):
        if not (isinstance(img, PIL.Image.Image)):
            if len(img.shape) > 3:
                img = tf.squeeze(img, axis=0)
        if title != None:
            plt.title(title)
        return plt.imshow(img)

    @staticmethod
    def tensor_to_image(tensor):
        tensor = tensor * 255
        tensor = np.array(tensor, dtype=np.uint8)
        if np.ndim(tensor) > 3:
            assert tensor.shape[0] == 1
            tensor = tensor[0]
        return PIL.Image.fromarray(tensor)

    @staticmethod
    def clip_0_1(clip_image):
        return tf.clip_by_value(clip_image, clip_value_min=0.0, clip_value_max=1.0)


class StyleContentModel(tf.keras.models.Model):
    def vgg_19_custom_model(self, layer_names):
        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
        vgg.trainable = False

        outputs = [vgg.get_layer(name).output for name in layer_names]

        model = tf.keras.Model([vgg.input], outputs)
        return model

    def __init__(self, style_layers, content_layers):
        super(StyleContentModel, self).__init__()
        self.vgg = self.vgg_19_custom_model(style_layers + content_layers)
        self.style_layers = style_layers
        self.content_layers = content_layers
        self.num_style_layers = len(style_layers)
        self.vgg.trainable = False

    def gram_matrix(self, input_tensor):
        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
        input_shape = tf.shape(input_tensor)
        num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)
        return result / (num_locations)

    def call(self, inputs):
        "Expects float input in [0,1]"
        inputs = inputs * 255.0
        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
        outputs = self.vgg(preprocessed_input)
        style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                          outputs[self.num_style_layers:])

        style_outputs = [self.gram_matrix(style_output)
                         for style_output in style_outputs]

        content_dict = {content_name: value
                        for content_name, value
                        in zip(self.content_layers, content_outputs)}

        style_dict = {style_name: value
                      for style_name, value
                      in zip(self.style_layers, style_outputs)}

        return {'content': content_dict, 'style': style_dict}


class NST():
    def __init__(self, content_img, style_img, style_w=1e-2, content_w=1e4, total_loss_var_w=30):
        self.content_img = content_img
        self.style_img = style_img
        self.style_w = style_w
        self.content_w = content_w
        self.total_loss_var_w = total_loss_var_w
        content_layers = ['block5_conv2']

        style_layers = ['block1_conv1',
                        'block2_conv1',
                        'block3_conv1',
                        'block4_conv1',
                        'block5_conv1']

        self.content_l_len = len(content_layers)
        self.style_l_len = len(style_layers)
        self.extract_from_vgg = StyleContentModel(style_layers, content_layers)

    def calculate_loss(self, outputs):  # outputs is a dicionary returned by outputs_from_vgg
        style_outputs = outputs["style"]
        content_outputs = outputs["content"]

        style_loss = tf.add_n(
            [tf.reduce_mean((style_outputs[name] - self.style_target[name]) ** 2) for name in style_outputs.keys()])
        style_loss *= self.style_w / self.style_l_len

        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - self.content_target[name]) ** 2) for name in
                                 content_outputs.keys()])
        content_loss *= self.content_w / self.content_l_len
        loss = style_loss + content_loss
        return loss

    @tf.function()
    def train_step(self, image, optimizer):
        # feedforward
        with tf.GradientTape() as tape:
            outputs = self.extract_from_vgg(image)
            loss = self.calculate_loss(outputs)
            loss += self.total_loss_var_w * tf.image.total_variation(image)
        # backprop
        grad = tape.gradient(loss, image)
        optimizer.apply_gradients([(grad, image)])
        image.assign(Image_Utils.clip_0_1(image))

    def predict(self, epochs=5, epoch_steps=100):
        # Targets of each image, which will be used for calculating the loss
        # which will be used to optimize the image with an Adam optimizer
        self.style_target = self.extract_from_vgg(self.style_img)["style"]
        self.content_target = self.extract_from_vgg(self.content_img)["content"]

        image = tf.Variable(self.content_img)
        optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
        for epoch in range(0, epochs):
            for step in range(0, epoch_steps):
                self.train_step(image, optimizer)
                print(".", end='')
            display.clear_output(wait=True)
            display.display(Image_Utils.tensor_to_image(image))
            print("Epoch: {}/{}".format(epoch + 1, epochs))
        return Image_Utils.tensor_to_image(image)


def NST_Video(path_in,path_out,style_img_path):
    clip = VideoFileClip(path_in, audio=False)
    video_writer = ffmpeg_writer.FFMPEG_VideoWriter(path_out, clip.size, clip.fps, codec="libx264",
                                                    preset="medium", bitrate="2000k",
                                                    audiofile=path_in, threads=None,
                                                    ffmpeg_params=None)
    style_img = Image_Utils.constrain_img(Image_Utils.load_img(style_img_path))
    array = []
    for frame in clip.iter_frames():
        array.append(frame)
        video_writer.write_frame(frame)
        nst = NST(Image_Utils.constrain_img(frame),style_img)
        image = nst.predict(epochs=100)
        plt.imshow(image)
        plt.show()
